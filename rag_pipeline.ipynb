{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1070cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LT\\.conda\\envs\\llmpractice\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf33ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 pages\n",
      "THE SCIENCE \n",
      "BEHIND HEAT\n",
      "Large Electric Stove with \n",
      "Flame Effect \n",
      "Instruction Manual\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"C:/wajahat/personal/learning/AI_agents_service_providers/stove.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages\")\n",
    "print(documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3888ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(documents[5].page_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7545c104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 223.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_num': 0,\n",
       "  'page_char_count': 84,\n",
       "  'page_word_count': 15,\n",
       "  'page_sentences_count': 1,\n",
       "  'page_tokens_count': 21.0,\n",
       "  'text': 'THE SCIENCE  BEHIND HEAT Large Electric Stove with  Flame Effect  Instruction Manual'},\n",
       " {'page_num': 1,\n",
       "  'page_char_count': 2009,\n",
       "  'page_word_count': 357,\n",
       "  'page_sentences_count': 19,\n",
       "  'page_tokens_count': 502.25,\n",
       "  'text': 'THE SCIENCE  BEHIND HEAT Getting Started Remove the appliance from the box.  Remove any packaging from the appliance.  Place the packaging inside the box and either store or dispose of safely. In the Box Large 2000W stove fire   Instruction manual Features Variable flame brightness control   Adjustable thermostat  Fan heater with two heat settings  Thermal safety cut-out  1000/2000W heat setting switches  Free standing Safety Instructions Do not use the appliance until it is fully assembled as described in this manual.  When using this electrical appliance, basic safety precautions should be followed to reduce the  risk of fire, electric shock and injury.  Check that the voltage indicated on the data plate corresponds with that of the local network  before connecting the appliance to the mains power supply.  This appliance is not intended for use by persons (including children) with reduced physical,  sensory or mental capabilities, or lack of experience and knowledge, unless they have been given  supervision or instruction concerning use of the appliance by a person responsible for their safety.  Children should be supervised to ensure that they do not play with the appliance.  Keep out of reach of children and do not allow them to operate this appliance.  Repairs to electrical appliances should only be performed by a qualified electrician. Improper  repairs may place the user at serious risk.  If the power supply cord is damaged, it must be replaced by a qualified engineer in order to  avoid a hazard.  Do not run the power cord under carpets, rugs, etc.  Do not allow the mains power cable to hang over sharp edges or come into contact with   hot surfaces.  Do not leave the appliance unattended whilst connected to the mains power supply.  Do not block the lower air intake of the appliance; place in an open, well ventilated space.  Do not use in a bathroom, bath tub or a swimming pool.  Do not use the appliance outdoors. BA488430HSMFOB Large Electric Stove with Flame Effect 3'},\n",
       " {'page_num': 2,\n",
       "  'page_char_count': 1350,\n",
       "  'page_word_count': 245,\n",
       "  'page_sentences_count': 20,\n",
       "  'page_tokens_count': 337.5,\n",
       "  'text': 'Do not use if you have wet hands.  Do not operate this appliance after a malfunction or after being dropped or damaged in any way.  Do not use this appliance with a programmer, timer, separate remote control system or any  other device that switches it on automatically, as a fire risk exists if it is covered or   positioned incorrectly.  The appliance must be positioned on a stable, level, heat-resistant surface.  The appliance must not be positioned directly under a mains power socket.  Never use accessories that are not recommended or supplied by the manufacturer. The   only user replacement items are the bulbs. All other servicing should be referred to a   qualified engineer.  Keep furniture, curtains and other flammable material at least 1 metre away from the appliance.  This appliance is intended for household use only and should not be used for industrial purposes.  Never immerse the appliance in water or any other liquid.  Never use the appliance on or near hot surfaces.  Warning: In order to avoid overheating, do not cover the appliance. Description of the Controls The switches to control the fire are located behind the doors of the stove on the right hand side. 1. Flame effect  2. Heat setting  3. Heat setting  4. Thermostatic control  5. Flame brightness BA488430HSMFOB Large Electric Stove with Flame Effect 4 1 2 3 4 5'},\n",
       " {'page_num': 3,\n",
       "  'page_char_count': 1887,\n",
       "  'page_word_count': 328,\n",
       "  'page_sentences_count': 21,\n",
       "  'page_tokens_count': 471.75,\n",
       "  'text': 'THE SCIENCE  BEHIND HEAT Instructions for Use Operating the Fire Plug the fire into the mains power supply and switch on if applicable.  Do not use any form of adaptor, extension lead or timer.  Press switch 1 to operate the flame effect and to turn the fan on without the heating function.  For a low level heating, press switch 2 for the 1000W setting.  For full heating turn on switches 2 and 3 for the 2000W setting.  Adjust the thermostat to finely regulate the temperature; turn it clockwise to increase the heat  output and anticlockwise to reduce it.  To vary the brightness of the flame effect, rotate the flame brightness dial clockwise to increase  the brightness and anticlockwise to lower the brightness level. Thermostat Instructions The sensitive hot air control (thermostat) will constantly maintain a pre-programmed  temperature due to its automatic operation.  Turn the temperature adjustment dial switch to the right, towards the maximum level.   Leave the switch in this position until the room reaches the desired temperature. Bulb Replacement This appliance is supplied with two 40 watt bulbs (E-12). When they require replacing, loosen  the two screws on the panel at the back of the fire.  Remove the panel and unscrew the bulbs.  Caution: There may be sharp edges; exercise extreme caution to avoid injury.  Always replace the bulbs with identical ones of the same wattage.  To reassemble, proceed in the reverse order. Cleaning and Maintenance Before attempting any cleaning or maintenance, unplug the fire from the mains power supply and  allow it to cool fully.  Clean the fire with a soft, damp, lint-free cloth and dry thoroughly.  Do not immerse the fire in water or any other liquid.   Do not use harsh or abrasive detergents or scourers. Technical Specification Supply: 220-240V~50Hz  Power: 2000W  BA488430HSMFOB Large Electric Stove with Flame Effect 5'},\n",
       " {'page_num': 4,\n",
       "  'page_char_count': 1767,\n",
       "  'page_word_count': 302,\n",
       "  'page_sentences_count': 14,\n",
       "  'page_tokens_count': 441.75,\n",
       "  'text': 'UP Global Sourcing  Manchester, OL9 0DD.  If this product does not reach you in an acceptable condition please contact our Customer  Services Department by one of the following methods: Telephone:\\t +44 (0)333 577 9820* *Telephone lines are open Monday – Friday, 9am – 5pm (Closed Bank Holidays) Email:\\t customersupport@beldray.co.uk  Fax:\\t 0161 628 2126\\t   Please have your delivery note to hand as details from it will be required. If you wish to return this product please return it to the retailer from where it was purchased  with your receipt (subject to their terms and conditions). Guarantee This product is guaranteed for a period of 1 year from the date of purchase against mechanical  and electrical defects. This guarantee is only valid if the appliance is used solely for domestic purposes in accordance  with the instructions and provided that it is not connected to an unsuitable electricity supply or  dismantled or interfered with in any way or damaged through misuse. Under this guarantee we  undertake to repair or replace free of charge any parts found to be defective. Reasonable proof  of purchase must be provided. Nothing in this guarantee or in the instructions relating to this product excludes, restricts or  otherwise affects your statutory rights.   In line with our policy of continuous development we reserve the right to change this product,  packaging and documentation specification without notice. Consumables are not guaranteed i.e. plug and fuse. The crossed out wheelie bin symbol on this item indicates that this appliance  needs to be disposed of in an environmentally friendly way when it becomes  of no further use or has worn out. Contact your local authority for details of  where to take the item for recycling. Guarantee 6'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_text_from_pdf(text: str) -> str:\n",
    "\n",
    "    cleaned_text = text.replace('\\n', \" \").strip()    \n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def open_read_pdf(pdf_path: str) -> List[Dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_text = []\n",
    "    for page_num, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = extract_text_from_pdf(text=text)\n",
    "        pages_and_text.append({ \"page_num\": page_num,\n",
    "                               \"page_char_count\": len(text),\n",
    "                               \"page_word_count\": len(text.split(\" \")),\n",
    "                               \"page_sentences_count\": len(text.split(\". \")),\n",
    "                               \"page_tokens_count\": len(text) / 4,\n",
    "                                 \"text\": text })\n",
    "        \n",
    "    return pages_and_text\n",
    "\n",
    "pages_and_text = open_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a0e93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LT\\AppData\\Local\\Temp\\ipykernel_24016\\2091549473.py:14: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored 15 chunks into Chroma DB at: C:/wajahat/personal/learning/AI_agents_service_providers/chroma_db/stove\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LT\\AppData\\Local\\Temp\\ipykernel_24016\\2091549473.py:18: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG ===\n",
    "pdf_path = \"C:/wajahat/personal/learning/AI_agents_service_providers/stove.pdf\"          # Path to your PDF\n",
    "chroma_dir = \"C:/wajahat/personal/learning/AI_agents_service_providers/chroma_db/stove\"                 # Folder to persist embeddings\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# === LOAD & SPLIT DOCUMENT ===\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "# === EMBEDDINGS ===\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# === VECTOR STORE (Chroma) ===\n",
    "vectorstore = Chroma.from_documents(docs, embedding_model, persist_directory=chroma_dir)\n",
    "vectorstore.persist()\n",
    "\n",
    "print(f\"✅ Stored {len(docs)} chunks into Chroma DB at: {chroma_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import ollama\n",
    "\n",
    "#1. Create a quantization config\n",
    "quantization_config = BitsAndBytesConfig( load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "attn_implementation = \"flash_attention_2\" if is_flash_attn_2_available() else \"sdpa\"\n",
    "\n",
    "#2. Model Id\n",
    "# model_id = \"openai-community/gpt2\"\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# model_path = \"ollama/llama2\"\n",
    "# client = ollama.Client()\n",
    "# model_path = llama2\n",
    "\n",
    "#3. Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "#4. Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                               quantization_config=quantization_config,\n",
    "                                               attn_implementation=attn_implementation,\n",
    "                                               low_cpu_mem_usage=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ddd5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.72s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     13\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_path)\n\u001b[32m     14\u001b[39m model = AutoModelForCausalLM.from_pretrained(model_path)\n\u001b[32m     16\u001b[39m generator = pipeline(\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     model=model,\n\u001b[32m     19\u001b[39m     torch_dtype=torch.float16,\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     device=\u001b[43mdevice\u001b[49m,\n\u001b[32m     21\u001b[39m     max_new_tokens=\u001b[32m512\u001b[39m,\n\u001b[32m     22\u001b[39m     temperature=\u001b[32m0.1\u001b[39m,\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m llm = HuggingFacePipeline(pipeline=generator)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# # === CREATE RETRIEVAL CHAIN ===\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# qa_chain = RetrievalQA.from_chain_type(\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#     llm=llm,\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#     retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#     return_source_documents=False,\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# === CONFIG ===\n",
    "model_path = \"ibm-granite/granite-4.0-micro\"  # or \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# === LOAD EMBEDDINGS & VECTORSTORE ===\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "# vectorstore = Chroma(persist_directory=chroma_dir, embedding_function=embedding_model)\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "# device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    # device=device,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# # === CREATE RETRIEVAL CHAIN ===\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "#     return_source_documents=False,\n",
    "# )\n",
    "\n",
    "print(\"✅ RAG pipeline ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bddf926e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m generator = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# device=device,\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m llm = HuggingFacePipeline(pipeline=generator)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# === CREATE RETRIEVAL CHAIN ===\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LT\\.conda\\envs\\llmpractice\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1059\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1058\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m load_tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1060\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1061\u001b[39m         tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LT\\.conda\\envs\\llmpractice\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1037\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1034\u001b[39m         tokenizer = config\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1036\u001b[39m         \u001b[38;5;66;03m# Impossible to guess what is the right tokenizer here\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m   1038\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mImpossible to guess which tokenizer to use. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1039\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1040\u001b[39m         )\n\u001b[32m   1042\u001b[39m \u001b[38;5;66;03m# Instantiate tokenizer if needed\u001b[39;00m\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokenizer, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[31mException\u001b[39m: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    # device=device,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# === CREATE RETRIEVAL CHAIN ===\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "model = f\"{model_path.replace('/', '_')}\"\n",
    "quries = \"stiching\"\n",
    "queries_path = f\"{quries}.json\"       # JSON file containing { \"questions\": [...], \"answers\": [...] }\n",
    "output_file = f\"{model_path}_{quries}_responses.txt\"\n",
    "\n",
    "# Example format for queries.json:\n",
    "# {\n",
    "#   \"questions\": [\n",
    "#       \"What is the main objective of the project?\",\n",
    "#       \"How long is the duration?\"\n",
    "#   ],\n",
    "#   \"answers\": [\n",
    "#       \"The main objective is to monitor and analyze energy usage patterns.\",\n",
    "#       \"The duration is one year.\"\n",
    "#   ]\n",
    "# }\n",
    "\n",
    "# === LOAD QUESTIONS & ANSWERS ===\n",
    "with open(queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "questions = data[\"questions\"]\n",
    "ground_truths = data[\"answers\"]\n",
    "\n",
    "# === EMBEDDING MODEL FOR SIMILARITY ===\n",
    "sim_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# === RUN INFERENCE LOOP ===\n",
    "results = []\n",
    "for i, (q, ref) in enumerate(zip(questions, ground_truths)):\n",
    "    print(f\"\\n[{i+1}/{len(questions)}] Question: {q}\")\n",
    "    model_answer = qa_chain.run(q)\n",
    "\n",
    "    # Compute similarity\n",
    "    emb_ref = sim_model.encode(ref, convert_to_tensor=True)\n",
    "    emb_ans = sim_model.encode(model_answer, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(emb_ref, emb_ans).item()\n",
    "\n",
    "    # Save to list\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"reference_answer\": ref,\n",
    "        \"model_answer\": model_answer,\n",
    "        \"similarity\": similarity\n",
    "    })\n",
    "\n",
    "# === SAVE TO TXT ===\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in results:\n",
    "        f.write(f\"Question: {r['question']}\\n\")\n",
    "        f.write(f\"Reference: {r['reference_answer']}\\n\")\n",
    "        f.write(f\"Model: {r['model_answer']}\\n\")\n",
    "        f.write(f\"Similarity: {r['similarity']:.4f}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"\\n✅ Done! Responses and similarities saved in {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmpractice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
